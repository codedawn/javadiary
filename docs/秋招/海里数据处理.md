**1、海量日志数据，提取出某日访问百度次数最多的那个IP。**

分而治之+hash

ip是32位，所以最多有2^32个ip，同样可以采用映射的方法，比如%1024，然后映射到1024个文件，

然后每个文件可以使用hashmap统计出现次数，纪律次数最多的ip，然后1024个数在比较



**2、搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。**

1千万条数据，一条255byte，255*10000000 ->kb->mb->gb  等于2.375gb

先使用hashmap统计，因为大概是300万，1G是可以放下的，然后维护一个10的小根堆，每次和根比较，比根大就弹出根，把新的数加入

o(n) + n*o(logk)

**3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。**

分治加hash，先取模分成小文件，然后hashmap统计

**4、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。**

重新取模%10，重新分配到10文件，然后还是分治加hash

**5、 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？**

50亿*64byte大概是320gb

分别对a和b取模%1000，相同的url肯定会在一个序号的文件a1,a2..,b1,b2

然后只要对应小文件求相同就可以了



容忍误差的话

或者是使用布隆过滤器，先用a映射，然后b过滤

**6、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。**

可以使用bitmap，一个数对应两位，一次就是01，两次就是10

或者是分治，取模，然后找出小文件不重复的数

**7、腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？**

bitmap

**8、怎么在海量数据中找出重复次数最多的一个？**

先hash，也就是取模到小文件，然后再统计小文件的最大，再汇总

**9、上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。**

**10、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。**

使用字典树统计每个词的出现的次数，时间复杂度o（n*length），然后用堆统计前10个词，是o（nlg10）

